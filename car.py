# -*- coding: utf-8 -*-
"""Car.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r1FEHJ_6biKWAElj-ZrO5zzB-IyNQwdM
"""

import pandas as pd
import re
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
train = pd.read_excel('Data_Train.xlsx')

train.shape

train.head()

# Obtaining the age of the car

# In presence of age attribute , Year is futile.......drop the feature

train['Age']  = 2019-train['Year']

train = train.drop(['Year'],axis=1)

# extracting the mileage

train['Mileage'] = train['Mileage'].str.extract('(\d+.\d+)')

# extracting the engine

train['Engine'] = train['Engine'].str.extract('(\d+)')

# extracting the power

train['Power'] = train['Power'].str.extract('(\d+.\d+)')

# extracting the manufacturer.....the importance of obtaining this feature will be known later

train['Manufacturer'] = [i.split(' ')[0] for i in train['Name']]

# mapping  plays a crucial role , as it determines the owner type

train['Owner_Type'] = train['Owner_Type'].map({'First':1,'Second':2,'Third':3,'Fourth & Above':4})

#converting type of mileage

train['Mileage'] = train['Mileage'].astype(float)

"""# New Feature"""

# Finding the fuel consumed per kilometers driven

train['Fuel_Consumed'] = train['Kilometers_Driven']/(train['Mileage'])

train.isnull().sum()

# Still there are some missing values............need to be filled

train['Fuel_Consumed'] = train.Fuel_Consumed.fillna(train.Fuel_Consumed.mean())

plt.scatter(x='Fuel_Consumed', y='Kilometers_Driven',data=train,edgecolors='w')
plt.title('Fuel Consumed Vs Distance Driven')
plt.xlabel('Fuel Consumption in litres')
plt.ylabel('Total Distance Driven')

# This is the perfect example of how data deceives us, if we have a glimpse we will realise that some values are far away 

# eventhough null-values are filled

# This phenomenon occurs due to the filling of values as 0 in Mileage which will raise our new feature to infinite

#  As outliers are invetable, we will ditch those values

# Removing the outliers based on the plot obtained

train = train.drop(train[(train['Fuel_Consumed']>22000) | (train['Mileage']<1)].index)

plt.scatter(x='Fuel_Consumed', y='Kilometers_Driven',data=train,edgecolors='w')
plt.title('Fuel Consumed Vs Distance Driven')
plt.xlabel('Fuel Consumption in litres')
plt.ylabel('Total Distance Driven')

# The results are staggering as there is huge change in plot

# The feature New_Price is the weak point of this..........eventhough it provides much of the data......

# Due to the presence of high volume of null values it is wise to drop the column

# Dropping the column with much NaN values

train = train.drop(['New_Price'],axis=1)

train.isnull().sum()

# Still there are null values

#Filling the Seats.......As an ideal car has 5 seats, better to fill with out any second thought

train['Seats'] = train['Seats'].fillna(5)

#Filling the values using median , as mean value is pointless, due to high amount of null values

train['Engine'] = train.groupby(['Manufacturer'])['Engine'].transform(lambda i:i.fillna(i.median()))

#Filling the power using Mean value

train.Power = train.groupby(['Manufacturer'])['Engine'].transform(lambda i:i.fillna(i.median()))

sb.kdeplot(train['Engine'])

train.head()

# Drop the Name, as it doesn't manner in anyways

train = train.drop(['Name'],axis =1)

from sklearn.preprocessing import LabelEncoder
label=LabelEncoder()

#Label Encoding the columns

for i in train.columns:
    
    if train[i].dtype=='object':
        
        train[i]=label.fit_transform(list(train[i].values))

train.head()

# Preserving the Target Variable

y = train['Price']

# Reckoning all the necessary data

X = train.drop(['Price'],axis = 1)

# Splitting the data

from sklearn.model_selection import train_test_split

# Considering the test size of 0.3 i.e., 30 % of data is taken under 

xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.3,random_state=42)

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression

linear_model = LinearRegression()

linear_model.fit(xtrain,ytrain)

linear_pred = linear_model.predict(xtest)

print(f'R2 Score of Linear Regression Model is {r2_score(ytest,linear_pred)}')
print('-'*100)
print(f'Mean Squared Error of Linear Regression Model is {mean_squared_error(ytest,linear_pred)}')
print('-'*100)
print(f'Mean Absolute Error of Linear Regression Model is {mean_absolute_error(ytest,linear_pred)}')

"""# Lasso"""

from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.1,max_iter=1500,warm_start=True)

lasso_model.fit(xtrain,ytrain)

lasso_pred = lasso_model.predict(xtest)

lasso_pred = lasso_model.predict(xtest)
print(f'R2 Score of Lasso Regression Model is {r2_score(ytest,lasso_pred)}')
print('-'*100)
print(f'Mean Squared Error of Lasso Regression Model is {mean_squared_error(ytest,lasso_pred)}')
print('-'*100)
print(f'Mean Absolute Error of Lasso Regression Model is {mean_absolute_error(ytest,lasso_pred)}')

"""# Decision Tree"""

from sklearn import tree

decision_tree = tree.DecisionTreeRegressor()

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

folds=KFold(n_splits=5,shuffle=True,random_state=100)

hyper_params=[{'criterion':['mse', 'friedman_mse', 'mae']}]

model=GridSearchCV(decision_tree,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

model.best_estimator_

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

folds=KFold(n_splits=5,shuffle=True,random_state=100)

hyper_params=[{'max_depth':[5,10,15,20,25,28]}]#,'min_samples_leaf':[20,30,50],'min_samples_split':[50,100,150]}]

model=GridSearchCV(decision_tree,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

folds=KFold(n_splits=5,shuffle=True,random_state=100)

hyper_params=[{'min_samples_leaf':[5,10,20,30,40,50,100,120,150,200,250,300]}]

model=GridSearchCV(decision_tree,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

folds=KFold(n_splits=5,shuffle=True,random_state=100)

hyper_params=[{'min_samples_split':[10,20,25,30,40,45,50,100,150,300,500]}]

model=GridSearchCV(decision_tree,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

from sklearn.tree import DecisionTreeRegressor

decision_tree = DecisionTreeRegressor(criterion='mae',max_depth=20,min_samples_leaf=10,min_samples_split=50)

decision_tree.fit(xtrain,ytrain)

decision_pred = decision_tree.predict(xtest)

print(f'R2 Score of Decision Tree Model is {r2_score(ytest,decision_pred)}')
print('-'*100)
print(f'Mean Squared Error of Decision Tree Model is {mean_squared_error(ytest,decision_pred)}')
print('-'*100)
print(f'Mean Absolute Error of Decision Tree Model is {mean_absolute_error(ytest,decision_pred)}')

"""# Random Forest"""

from sklearn.ensemble import RandomForestRegressor

random_forest = RandomForestRegressor()

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import KFold

folds=KFold(n_splits=5,shuffle=True,random_state=100)

hyper_params=[{'min_samples_split':[10,20,25,30,40,45,50,100,150,300,500]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'criterion':['mse', 'friedman_mse', 'mae']}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'n_estimators':[10,20,30,40,50,60,70,80,90,100,150,200,250,300,400,450,500]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'min_samples_leaf':[10,20,30,40,50,60,70,80,90,100,150,200,250,300,400,450,500]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'min_impurity_decrease':[10,20,30,40,50,60,70,80,90,100,150,200,250,300,400,450,500]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'min_weight_fraction_leaf':[0.05,0.5,1,2]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

hyper_params=[{'max_features':[0.05,0.5,1,2,5,10,20,30,40,50,100,200,500,1000]}]

model=GridSearchCV(random_forest,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

random_forest = RandomForestRegressor(criterion='mse',n_estimators=100,min_samples_leaf=1)

random_forest = RandomForestRegressor()

random_forest.fit(xtrain,ytrain)

random_forest_pred = random_forest.predict(xtest)

print(f'R2 Score of Random Forest Model is {r2_score(ytest,random_forest_pred)}')
print('-'*100)
print(f'Mean Squared Error of Random Forest Model is {mean_squared_error(ytest,random_forest_pred)}')
print('-'*100)
print(f'Mean Absolute Error of Random Forest Model is {mean_absolute_error(ytest,random_forest_pred)}')

"""XG Boost"""

import xgboost as xgb
XGB_model = xgb.XGBRegressor()

hyper_params=[{'n_jobs':[0.05,0.5,1,2,5]}]

model=GridSearchCV(XGB_model,param_grid=hyper_params,cv=folds)

model.fit(xtrain,ytrain)

model.best_params_

XGB_model = xgb.XGBRegressor(n_estimators = 1500, 
                            max_depth = 8,
                            colsample_bylevel = 0.9,
                            learning_rate = 0.1,
                            random_state=12
                             ,eta = 0.1,num_round = 500)

XGB_model.fit(xtrain, ytrain)

xg_pred = XGB_model.predict(xtest)

print(f'R2 Score of XG Boost is {r2_score(ytest,xg_pred)}')
print('-'*100)
print(f'Mean Squared Error of XG Boost is {mean_squared_error(ytest,xg_pred)}')
print('-'*100)
print(f'Mean Absolute Error of XG Boost is {mean_absolute_error(ytest,xg_pred)}')